[2025-09-03T19:42:20.979+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: streaming_pipeline.bronze_streaming_continuous manual__2025-09-03T19:42:19.548175+00:00 [queued]>
[2025-09-03T19:42:20.986+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: streaming_pipeline.bronze_streaming_continuous manual__2025-09-03T19:42:19.548175+00:00 [queued]>
[2025-09-03T19:42:20.986+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2025-09-03T19:42:20.986+0000] {taskinstance.py:1280} INFO - Starting attempt 1 of 1
[2025-09-03T19:42:20.986+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2025-09-03T19:42:20.995+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): bronze_streaming_continuous> on 2025-09-03 19:42:19.548175+00:00
[2025-09-03T19:42:20.997+0000] {standard_task_runner.py:55} INFO - Started process 380 to run task
[2025-09-03T19:42:21.000+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'streaming_pipeline', 'bronze_streaming_continuous', 'manual__2025-09-03T19:42:19.548175+00:00', '--job-id', '9', '--raw', '--subdir', 'DAGS_FOLDER/dag_streaming.py', '--cfg-path', '/tmp/tmpkmo5h2hg']
[2025-09-03T19:42:21.001+0000] {standard_task_runner.py:83} INFO - Job 9: Subtask bronze_streaming_continuous
[2025-09-03T19:42:21.022+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/***/settings.py:249 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2025-09-03T19:42:21.060+0000] {task_command.py:388} INFO - Running <TaskInstance: streaming_pipeline.bronze_streaming_continuous manual__2025-09-03T19:42:19.548175+00:00 [running]> on host e954bc4828da
[2025-09-03T19:42:21.115+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Klaus_Rezende
AIRFLOW_CTX_DAG_ID=streaming_pipeline
AIRFLOW_CTX_TASK_ID=bronze_streaming_continuous
AIRFLOW_CTX_EXECUTION_DATE=2025-09-03T19:42:19.548175+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2025-09-03T19:42:19.548175+00:00
[2025-09-03T19:42:21.116+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2025-09-03T19:42:21.117+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'python3 /opt/***/scripts/bronze/tb_bz_streaming_continuous.py']
[2025-09-03T19:42:21.122+0000] {subprocess.py:86} INFO - Output:
[2025-09-03T19:42:22.833+0000] {subprocess.py:93} INFO - :: loading settings :: url = jar:file:/usr/local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-09-03T19:42:22.884+0000] {subprocess.py:93} INFO - Ivy Default Cache set to: /root/.ivy2/cache
[2025-09-03T19:42:22.884+0000] {subprocess.py:93} INFO - The jars for the packages stored in: /root/.ivy2/jars
[2025-09-03T19:42:22.897+0000] {subprocess.py:93} INFO - org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2025-09-03T19:42:22.898+0000] {subprocess.py:93} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-7b921403-b19c-4e26-8023-f43326d8bbe0;1.0
[2025-09-03T19:42:22.898+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-09-03T19:42:25.816+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
[2025-09-03T19:42:26.231+0000] {subprocess.py:93} INFO - 	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[2025-09-03T19:42:26.512+0000] {subprocess.py:93} INFO - 	found org.apache.kafka#kafka-clients;3.4.1 in central
[2025-09-03T19:42:26.805+0000] {subprocess.py:93} INFO - 	found org.lz4#lz4-java;1.8.0 in central
[2025-09-03T19:42:27.095+0000] {subprocess.py:93} INFO - 	found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2025-09-03T19:42:28.381+0000] {subprocess.py:93} INFO - 	found org.slf4j#slf4j-api;2.0.7 in central
[2025-09-03T19:42:30.718+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2025-09-03T19:42:31.018+0000] {subprocess.py:93} INFO - 	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2025-09-03T19:42:32.220+0000] {subprocess.py:93} INFO - 	found commons-logging#commons-logging;1.1.3 in central
[2025-09-03T19:42:32.498+0000] {subprocess.py:93} INFO - 	found com.google.code.findbugs#jsr305;3.0.0 in central
[2025-09-03T19:42:35.206+0000] {subprocess.py:93} INFO - 	found org.apache.commons#commons-pool2;2.11.1 in central
[2025-09-03T19:43:25.962+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...
[2025-09-03T19:42:35.675+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar (465ms)
[2025-09-03T19:42:35.820+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...
[2025-09-03T19:42:35.962+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (286ms)
[2025-09-03T19:42:36.115+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[2025-09-03T19:42:36.682+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (719ms)
[2025-09-03T19:42:36.819+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2025-09-03T19:42:36.954+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (272ms)
[2025-09-03T19:42:37.094+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2025-09-03T19:42:37.234+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (279ms)
[2025-09-03T19:42:37.372+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2025-09-03T19:42:38.797+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (1563ms)
[2025-09-03T19:42:38.939+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2025-09-03T19:42:39.084+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (287ms)
[2025-09-03T19:42:39.238+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2025-09-03T19:42:39.396+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (312ms)
[2025-09-03T19:42:39.532+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2025-09-03T19:42:39.670+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (273ms)
[2025-09-03T19:42:39.852+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2025-09-03T19:42:40.837+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (1167ms)
[2025-09-03T19:42:40.983+0000] {subprocess.py:93} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2025-09-03T19:42:41.127+0000] {subprocess.py:93} INFO - 	[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (289ms)
[2025-09-03T19:42:41.127+0000] {subprocess.py:93} INFO - :: resolution report :: resolve 12312ms :: artifacts dl 5917ms
[2025-09-03T19:42:41.127+0000] {subprocess.py:93} INFO - 	:: modules in use:
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	commons-logging#commons-logging;1.1.3 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	org.apache.commons#commons-pool2;2.11.1 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	org.lz4#lz4-java;1.8.0 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	org.slf4j#slf4j-api;2.0.7 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	|                  |            modules            ||   artifacts   |
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |
[2025-09-03T19:42:41.128+0000] {subprocess.py:93} INFO - 	---------------------------------------------------------------------
[2025-09-03T19:42:41.131+0000] {subprocess.py:93} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-7b921403-b19c-4e26-8023-f43326d8bbe0
[2025-09-03T19:42:41.131+0000] {subprocess.py:93} INFO - 	confs: [default]
[2025-09-03T19:42:41.198+0000] {subprocess.py:93} INFO - 	11 artifacts copied, 0 already retrieved (56767kB/66ms)
[2025-09-03T19:42:41.340+0000] {subprocess.py:93} INFO - 25/09/03 19:42:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-09-03T19:42:41.474+0000] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2025-09-03T19:42:41.474+0000] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-09-03T19:42:43.695+0000] {subprocess.py:93} INFO - 25/09/03 19:42:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-306eda0b-aa60-4823-8e15-596064e229d3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2025-09-03T19:42:43.709+0000] {subprocess.py:93} INFO - 25/09/03 19:42:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2025-09-03T19:42:44.032+0000] {subprocess.py:93} INFO - 25/09/03 19:42:44 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2025-09-03T19:42:53.122+0000] {subprocess.py:93} INFO - 25/09/03 19:42:53 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors
[2025-09-03T19:46:53.596+0000] {subprocess.py:93} INFO - 25/09/03 19:46:53 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 53594 milliseconds
[2025-09-03T19:54:36.962+0000] {subprocess.py:93} INFO - [Stage 25:>                                                         (0 + 1) / 1]                                                                                25/09/03 19:54:36 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 56962 milliseconds
[2025-09-03T20:01:26.674+0000] {subprocess.py:93} INFO - [Stage 49:>                                                         (0 + 1) / 1]                                                                                [Stage 54:>                                                         (0 + 1) / 1]                                                                                [Stage 60:>                                                         (0 + 1) / 1]                                                                                25/09/03 20:01:26 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.16.240.7:29092) could not be established. Broker may not be available.
[2025-09-03T20:01:26.774+0000] {subprocess.py:93} INFO - 25/09/03 20:01:26 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 1 (kafka/172.16.240.7:29092) could not be established. Broker may not be available.
